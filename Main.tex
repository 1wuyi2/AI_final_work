% https://www.overleaf.com/read/jydxqkkkskzp
% https://github.com/MCG-NKU/NSFC-LaTex
% by Ming-Ming Cheng https://mmcheng.net

%\documentclass[12pt]{article}
\documentclass[12pt,AutoFakeBold]{article}
\usepackage[UTF8]{ctex}
\usepackage{nsfc}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{caption} 
\usepackage{tabularx}   
\usepackage{ragged2e}   
\usepackage{geometry}   
\geometry{left=2.5cm, right=2.5cm, top=2cm, bottom=2cm}  % 扩大边距，避免表格溢出
\newcommand{\cmm}[1]{\textcolor[rgb]{0,0.6,0}{CMM: #1}}
\newcommand{\todo}[1]{{\textcolor{red}{\bf [#1]}}}
\newcommand{\myPara}[1]{\paragraph{#1：}}

% yaxing: begin
\usepackage[resetlabels]{multibib}
\newcites{sec}{Reference}
\usepackage{color}

\newcommand{\upcite}[1]{{\textsuperscript{\cite{#1}}}}
\newcommand{\sprod}[2]{\left\langle #1,#2 \right\rangle}
\newcommand{\minisection}[1]{\vspace{0.04in}\noindent {\bf #1}\ \ }


%  \iffalse
\newcommand{\revision}[1]{{\color{blue} #1}}
\graphicspath{{figures/}}


\begin{document}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\linewidth]{figures/logo.jpeg}
\end{figure}

\vspace{1cm}
\hrulefill
    \begin{center}
        \Huge\textbf{人工智能导论大作业-\\ 图像生成探究报告 }
     
    \end{center}   

\hrulefill
\vspace{1cm}

\begin{table}[h]
    \centering
    \large
    \begin{tabular}{ll}
    \textbf{姓名：高一飞   \quad  学号:2411862} \quad 论文1  \\
    \textbf{姓名：谭锦睿    \quad  学号:2412771} \quad 论文2 \\
    \textbf{姓名：王毅豪    \quad  学号:2412509} \quad 论文3 \\
    \end{tabular}
\end{table}

\newpage

%%%%%%%%% TITLE

\title{\kaishu 作业正文}

\maketitle




\section{问题描述}


\section{核心内容}


\subsection{Deep Residual Learning for Image Recognition}
\subsubsection{论文动机}

深度神经网络在图像识别任务中取得了突破性进展，网络深度成为提升模型性能的关键因素。然而，当网络深度显著增加时，研究者观察到一种“退化问题”（degradation problem）：随着网络层数增多，训练误差不仅没有下降，反而开始上升（如图1所示），这种现象并非由于过拟合引起，而是由于极深网络的优化困难。尽管通过批归一化（Batch Normalization）和恰当的初始化方法可以缓解梯度消失/爆炸问题，但深度网络的训练误差饱和甚至恶化的问题依然存在。

作者指出，理论上，一个更深的网络至少应该能够达到与较浅网络相同的性能——只需将新增的层实现为恒等映射（identity mapping），其余层复制浅层网络的权重即可。然而实验表明，现有的优化算法（如SGD）难以在合理时间内将多层非线性层训练为近似恒等映射。这一现象促使作者重新思考深度网络的构建方式：与其让堆叠的层直接拟合一个复杂的目标映射 $\mathcal{H}(\mathbf{x})$，不如让它们拟合残差映射 $\mathcal{H}(\mathbf{x}) - \mathbf{x}$，从而简化优化过程，使极深网络的训练成为可能。

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/degradation.png}  
    \caption{\small 20层与56层普通网络在CIFAR-10上的训练误差（左）与测试误差（右）。更深网络表现出更高的训练误差。}
    \label{fig:degradation}
\end{figure}

\subsubsection{方法}

本文的核心创新是提出了“残差学习框架”（Residual Learning Framework）。对于一个由若干层组成的模块，我们不再直接学习目标映射 $\mathcal{H}(\mathbf{x})$，而是学习残差函数 $\mathcal{F}(\mathbf{x}) = \mathcal{H}(\mathbf{x}) - \mathbf{x}$，然后将原始输入 $\mathbf{x}$ 与残差输出相加，得到最终输出 $\mathbf{y} = \mathcal{F}(\mathbf{x}) + \mathbf{x}$。这种结构可以通过引入“快捷连接”（shortcut connection）实现，即从输入直接跳过多层连接到输出，并进行逐元素相加（如图2所示）。

数学表达上，对于一个残差单元，其计算过程为：

\begin{equation}
\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathbf{x}
\label{eq:residual}
\end{equation}

其中 $\mathcal{F}(\mathbf{x}, \{W_i\})$ 表示待学习的残差映射，通常由若干卷积层、批归一化层和ReLU激活函数组成。当输入与输出维度不一致时，可通过一个线性投影 $W_s$ 来匹配维度：

\begin{equation}
\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + W_s\mathbf{x}
\label{eq:residual_proj}
\end{equation}

作者设计了两种主要的残差网络结构：一种使用两个 $3\times3$ 卷积层的基本块（用于ResNet-34），另一种使用“瓶颈结构”（bottleneck），由 $1\times1$、$3\times3$、$1\times1$ 三个卷积层组成，用于构建更深的ResNet-50/101/152。瓶颈结构在减少计算量的同时保持了网络的表达能力。

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/resnet_block.png}  
    \caption{\small 残差学习基本单元（左）与瓶颈结构单元（右）}
    \label{fig:res_blocks}
\end{figure}

\subsubsection{实验结果}

作者在多个数据集和任务上验证了残差网络的有效性：

1. \textbf{ImageNet分类：} 在ImageNet 2012数据集上，152层ResNet取得了当时最优的单模型性能，top-5错误率仅为4.49\%，且计算复杂度低于VGG-16/19。通过模型集成，错误率进一步降至3.57\%，赢得ILSVRC 2015分类竞赛冠军（表1）。

2. \textbf{CIFAR-10分析：} 在CIFAR-10上，残差网络成功训练了110层乃至1202层的极深网络，未出现退化现象。110层ResNet仅用1.7M参数就达到了6.43\%的错误率，优于同类方法（表2）。层响应分析表明，残差网络中学到的残差函数值通常较小，验证了恒等映射作为优化起点的有效性。

3. \textbf{目标检测与分割：} 将ResNet-101作为Faster R-CNN的特征提取器，在PASCAL VOC和MS COCO检测任务上均显著超越基于VGG-16的基线，特别是在COCO上mAP@[.5,.95]相对提升达28\%。基于残差网络的方法在ILSVRC \& COCO 2015竞赛中赢得了检测、定位和分割全部四项任务的第一名。

4. \textbf{极深网络训练：} 作者成功训练了超过1000层的残差网络（1202层），在CIFAR-10上仍能收敛至极低的训练误差，证明了残差学习框架对优化难度的根本性解决。

\begin{table}[h]
  \centering
  \begin{tabular}{@{}lcc@{}}
    \hline \hline
    模型 & 层数 & Top-5错误率（\%） \\
    \hline
    VGG-16 & 16 & 7.32 \\
    GoogLeNet & 22 & 6.66 \\
    ResNet-34 & 34 & 5.71 \\
    ResNet-50 & 50 & 5.25 \\
    ResNet-101 & 101 & 4.60 \\
    ResNet-152 & 152 & \textbf{4.49} \\
    ResNet集成模型 & - & \textbf{3.57} \\
    \hline \hline
  \end{tabular}
  \caption{ImageNet 2012分类任务错误率对比}
  \label{tab:imagenet_results}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{@{}lccc@{}}
    \hline \hline
    方法 & 层数 & 参数量 & 错误率（\%） \\
    \hline
    Highway Network & 19 & 2.3M & 7.54 \\
    FitNet & 19 & 2.5M & 8.39 \\
    ResNet-20 & 20 & 0.27M & 8.75 \\
    ResNet-56 & 56 & 0.85M & 6.97 \\
    ResNet-110 & 110 & 1.7M & \textbf{6.43} \\
    ResNet-1202 & 1202 & 19.4M & 7.93 \\
    \hline \hline
  \end{tabular}
  \caption{CIFAR-10分类任务结果对比}
  \label{tab:cifar_results}
\end{table}








\subsection{Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}
\subsubsection{论文动机}
在在计算机视觉任务中，建模一直由卷积神经网络(CNN)占据主导地位。从AlexNet开始，CNN架构随着其不断增大的规模，更广泛的连接以及更复杂的卷积形式而不断增强。

而在NLP领域中，利用注意力机制来建模长程依赖关系的Transformer是当今的主流架构。故研究人员致力于探索将其应用于计算机视觉领域。

论文指出由于语言领域和视觉领域的模态差异，传统Transformer对于适配视觉领域面临两大核心挑战：一是视觉实体的尺度差异极大，而文本中单词的尺度固定；二是图像像素分辨率远高于文本单词数量，传统Transformer的全局自注意力计算复杂度与token数量呈二次关系，在高分辨率图像或语义分割等密集预测任务中难以落地。

为解决这些差异，论文中提出一种基于“移位窗口”（Shifted Windows）计算特征表示的分层Transformer。该移位窗口机制通过将自注意力计算限制在非重叠的局部窗口内，同时支持跨窗口连接，大幅提升了效率。这种分层架构能够灵活建模不同尺度的视觉信息，且计算复杂度与图像尺寸呈线性关系。

\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/figure1.png}  
    \caption{\small (a)Swin Transformer通过深层合并图像补丁(灰色部分)构建分层特征图，仅在局部窗口(红色部分)内计算自注意力。其中计算复杂度与输入图像尺寸呈线性关系。
                    (b)与改进后相对比，此前的Transformer只生成单一低分辨率特征图，而其全局自注意力的计算导致计算复杂度与图像尺寸呈二次关系。}
    \label{fig:hierarchical_feature}
\end{figure*}

\subsubsection{核心方法}
Swin Transformer的主要创新点围绕“分层架构”以及“移位窗口自注意力”展开，通过模块化的设计兼顾了效率与性能，以下是我对关键方法的分析与理解：

\paragraph{1.整体分层架构：用于适配多尺度任务}
论文借鉴CNN的多尺度特征设计，将输入图像通过“补丁分割-线性嵌入-补丁合并”的流程生成分层特征图，具体流程如下：

补丁分割：将输入RGB图像划分为4×4非重叠补丁，每个补丁的原始特征维度为$4×4×3=48$；

线性嵌入：通过线性层将48维补丁特征投影至自定义维度$C$（如Swin-T中$C=96$）；

分层Stage：共设计4个Stage，通过“补丁合并层”逐步下采样：
  
- Stage 1：保持$\frac{H}{4} \times \frac{W}{4}$分辨率，仅通过Swin Transformer块进行特征变换；

- Stage 2~4：每个Stage先通过2×2补丁合并（将相邻4个补丁的特征拼接后线性投影），使token数量减少4倍、分辨率下采样2倍，最终输出$\frac{H}{8} \times \frac{W}{8}$、$\frac{H}{16} \times \frac{W}{16}$、$\frac{H}{32} \times \frac{W}{32}$三种分辨率特征图。

这种设计让Swin Transformer的特征输出与传统CNN（如ResNet）完全兼容，可直接替换现有视觉任务的骨干网络，极大降低了应用门槛。

\paragraph{2.移位窗口自注意力}
这是论文最核心的创新点，主要解决了传统Transformer的二次复杂度问题

 局部窗口自注意力（W-MSA）：将特征图划分为$M×M$（默认$M=7$）的非重叠窗口，仅在窗口内计算自注意力。通过对比全局自注意力与窗口自注意力的复杂度公式，能清晰看到其效率优势：
\vspace{-3pt}
\begin{equation}
\Omega(MSA)=4hwC^{2}+2(hw)^{2}C,
\label{eq:global_msa}
\end{equation}
\vspace{-8pt}
\begin{equation}
\Omega(W\text{-MSA})=4hwC^{2}+2M^{2}hwC,
\label{eq:window_msa}
\end{equation}
其中$hw$为token数量，$C$为特征维度。当$M$固定时，窗口自注意力的复杂度与图像尺寸呈线性关系，彻底解决了高分辨率图像的计算瓶颈。

- 移位窗口（SW-MSA）：为了打破窗口间的信息隔离，论文设计了“连续块窗口移位”策略——第$l$层采用常规划窗，第$l+1$层窗口向右向下移位$\lfloor\frac{M}{2}\rfloor$像素，使新窗口跨越前一层的窗口边界，建立跨窗口连接。连续块的计算过程如下：
\vspace{-3pt}
\begin{equation}
\begin{aligned}
\hat{z}^{l}&=W\text{-MSA}\left(LN\left(z^{l-1}\right)\right)+z^{l-1}, \\
z^{l}&=MLP\left(LN\left(\hat{z}^{l}\right)\right)+\hat{z}^{l}, \\
\hat{z}^{l+1}&=SW\text{-MSA}\left(LN\left(z^{l}\right)\right)+z^{l}, \\
z^{l+1}&=MLP\left(LN\left(\hat{z}^{l+1}\right)\right)+\hat{z}^{l+1},
\end{aligned}
\label{eq:swin_block}
\end{equation}

- 相对位置偏置：视觉任务中，物体的相对位置比绝对位置更重要。论文在自注意力计算中引入相对位置偏置$B$，捕捉窗口内补丁的相对位置关系，公式如下：
\vspace{-3pt}
\begin{equation}
Attention(Q, K, V)=SoftMax\left(\frac{QK^T}{\sqrt{d}}+B\right)V,
\label{eq:relative_attention}
\end{equation}
其中$B$由更小的矩阵$\hat{B} \in \mathbb{R}^{(2M-1) \times (2M-1)}$参数化，既减少了参数量，又能通过插值适配不同窗口尺寸。

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/figure2.png}  
    \caption{\small 移位窗口自注意力示意图：层$l$（左）采用常规划分窗口，层$l+1$（右）采用移位窗口，新窗口跨越前一层窗口边界，建立跨窗口连接。}
    \label{fig:shifted_window}
\end{figure*}

- 高效批量计算：移位窗口会导致窗口数量增加，论文通过“循环移位+掩码机制”优化——将特征图循环移位后按常规窗口划分，再通过掩码限制仅在原窗口内计算注意力，避免了计算冗余，降低了推理延迟。

\paragraph{3. 架构变体：适配不同复杂度需求}
论文设计了4种不同规模的变体，方便根据任务需求选择，核心参数如下：
\begin{itemize}
    \item Swin-T（tiny）：$C=96$，层数$\{2,2,6,2\}$，对标ResNet-50，适合轻量型任务；
    \item Swin-S（small）：$C=96$，层数$\{2,2,18,2\}$，对标ResNet-101，平衡性能与效率；
    \item Swin-B（base）：$C=128$，层数$\{2,2,18,2\}$，对标ViT-B/DeiT-B，适合高精度任务；
    \item Swin-L（large）：$C=192$，层数$\{2,2,18,2\}$，复杂度为Swin-B的2倍，追求极致性能。
\end{itemize}

\subsubsection{三、实验结果分析与思考}
论文在图像分类、目标检测、语义分割三大核心任务中验证了模型性能，以下是我对实验结果的关键分析：

\paragraph{1. 图像分类任务（ImageNet-1K）}
实验设置了“仅ImageNet-1K训练”和“ImageNet-22K预训练+ImageNet-1K微调”两种场景，核心结果如下表所示：

\begin{table}[h]
  \centering
  \begin{tabular}{@{}lccccc@{}}
    \hline
    \hline
    Method & Image Size & \#Param. & FLOPs & Throughput (img/s) & ImageNet Top-1 Acc. \\
    \hline
    ViT-B/16 & $384^2$ & 86M & 55.4G & 85.9 & 84.0\% \\
    Swin-B & $384^2$ & 88M & 47.0G & 84.7 & 86.4\% \\
    Swin-L & $384^2$ & 197M & 103.9G & 42.1 & 87.3\% \\
    \hline
    \hline
  \end{tabular}
  \caption{ImageNet-1K分类实验结果（ImageNet-22K预训练）。Swin Transformer在相同复杂度下显著超越ViT，Swin-L达到87.3\%的Top-1准确率。}
  \label{tab:imagenet_result}
\end{table}

分析：Swin-B与ViT-B参数量相近，但Top-1准确率高2.4%，且FLOPs低15.2%，说明分层架构和移位窗口的设计更贴合视觉任务特性；Swin-L达到87.3%的Top-1准确率，证明模型具有良好的 scalability。

\paragraph{2. 目标检测与实例分割任务（COCO 2017）}
实验采用Cascade Mask R-CNN、ATSS等4种主流框架，核心系统级对比结果如下：

\begin{table}[h]
  \centering
  \begin{tabular}{@{}lcccccc@{}}
    \hline
    \hline
    Method & Backbone & Test Set & Box AP & Mask AP & \#Param. & FLOPs \\
    \hline
    Copy-paste & - & COCO test-dev & 56.0 & 47.4 & 185M & 1440G \\
    DetectoRS & - & COCO test-dev & 55.7 & 48.5 & - & - \\
    Swin-L (HTC++) & - & COCO test-dev & 58.7 & 51.1 & 284M & 1470G \\
    \hline
    \hline
  \end{tabular}
  \caption{COCO目标检测与实例分割系统级对比结果。Swin-L超越此前SOTA，Box AP提升+2.7，Mask AP提升+2.6。}
  \label{tab:coco_result}
\end{table}

分析：Swin-L的Box AP和Mask AP分别比此前的SOTA方法提升+2.7和+2.6，证明分层特征图能有效支撑密集预测任务；对比ResNet-50，Swin-T在相同框架下平均提升3.4~4.2 Box AP，说明Transformer的建模能力优于CNN。

\paragraph{3. 语义分割任务（ADE20K）}
采用UperNet框架进行实验，核心结果如下：

\begin{table}[h]
  \centering
  \begin{tabular}{@{}lcccc@{}}
    \hline
    \hline
    Method & Backbone & Pre-training & \#Param. & ADE20K val mIoU \\
    \hline
    SETR-T-Large & - & ImageNet-22K & 308M & 50.3\% \\
    Swin-S & UperNet & ImageNet-1K & 81M & 49.3\% \\
    Swin-L & UperNet & ImageNet-22K & 234M & 53.5\% \\
    \hline
    \hline
  \end{tabular}
  \caption{ADE20K语义分割实验结果。Swin-L超越此前SOTA，mIoU提升+3.2，模型尺寸更优。}
  \label{tab:ade20k_result}
\end{table}

分析：Swin-L的mIoU比SETR-T-Large高3.2个百分点，且参数量仅为后者的76%，说明Swin Transformer的特征提取效率更高；Swin-S在仅使用ImageNet-1K预训练的情况下，mIoU达到49.3%，优于同期多数模型，证明其泛化能力较强。

\paragraph{4. 消融实验：关键设计的有效性验证}
论文通过消融实验验证了核心设计的必要性，结果如下：

\begin{table}[h]
  \centering
  \begin{tabular}{@{}lccccc@{}}
    \hline
    \hline
    Design Element & ImageNet Top-1 & COCO Box AP & COCO Mask AP & ADE20K mIoU \\
    \hline
    w/o shifting & 80.2\% & 47.7 & 41.5 & 43.3\% \\
    shifted windows & 81.3\% (+1.1\%) & 50.5 (+2.8) & 43.7 (+2.2) & 46.1\% (+2.8) \\
    abs. pos. embedding & 80.5\% & 49.0 & 42.4 & 43.2\% \\
    rel. pos. bias & 81.3\% (+0.8\%) & 50.5 (+1.5) & 43.7 (+1.3) & 46.1\% (+2.9) \\
    \hline
    \hline
  \end{tabular}
  \caption{关键设计消融实验结果（Swin-T）。移位窗口和相对位置偏置显著提升多任务性能。}
  \label{tab:ablation_result}
\end{table}

分析：移除“移位窗口”后，三大任务性能均明显下降，证明跨窗口连接对建模能力的重要性；“相对位置偏置”比“绝对位置嵌入”更适配视觉任务，尤其是在检测和分割任务中优势明显，这让我理解到视觉建模更注重“相对关系”而非“绝对坐标”。











\subsection{MAGE: MAsked Generative Encoder to UnifyRepresentation Learning and Image Synthesi}
\subsubsection{论文动机}
1.	任务割裂问题：生成建模与表征学习是计算机视觉核心任务，但长期独立训练，既浪费互补潜力，又增加训练维护成本。

2.	结构差异挑战：生成是 “低维输入→高维输出”，表征学习是 “高维图像→低维嵌入”，计算机视觉领域缺乏类似 NLP 中 BERT 的统一框架。

3.	现有 MIM 局限：基于掩码图像建模（MIM）的表征方法采用像素级重建损失，导致图像重建模糊，无法兼顾生成质量。
\subsubsection{方法}
1. 核心创新点

•	可变掩码率策略：掩码率从 0.5-1 的截断高斯分布采样，高掩码率（接近 1）适配生成任务（100 \% 掩码图像重建），低掩码率适配表征学习（0\% 掩码图像编码），共享架构与训练方案。

•	语义令牌化：用预训练 VQGAN 将图像量化为 16×16 语义令牌，输入与重建目标均为令牌（非像素），解决重建模糊问题。

2. 模型架构与训练
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/屏幕截图 2025-12-25 101129.png}
    \caption{论文 3 figure 3 }
\end{figure}
 
•	架构：ViT 编码器 - 解码器，输入令牌经掩码与丢弃后，拼接 “伪类别令牌”，编码器输出用图像专属类别令牌填充，解码器重建原始令牌。

•	损失函数：核心为重建交叉熵损失（仅优化被掩码令牌），可选添加对比损失（MAGE-C），无需额外数据增强。

•	生成推理：20 步迭代解码，按余弦调度替换低置信度掩码令牌。

3. 预训练设置

•	输入分辨率 256×256，令牌序列长度 256，预训练 ViT-B/ViT-L 模型。

•	优化器 AdamW，训练 1600 个 epoch，余弦学习率调度 + 80 个 epoch 预热。

\subsubsection{实验结果}

1. 核心性能总览

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/屏幕截图 2025-12-25 101107.png}
    \caption*{论文 3 figure 1}
\end{figure}

•	论文图 1 直观展示 MAGE 在双任务的 SOTA 地位：ViT-L 模型无类别生成 FID=9.10，线性探测准确率 = 78.9\%，大幅超越 MaskGIT、MAE 等基线。

2. 无类别条件图像生成

\begin{table}[htbp]
    \centering
    \caption*{论文3 表 1 无类别条件图像生成性能对比}
    \begin{tabular}{llrr}
        \toprule
        方法 & 模型 & FID & IS \\
        \midrule
        MaskGIT（此前 SOTA） & BERT & 20.68 & 42.08 \\
        MAGE & ViT-B & 11.11 & 81.17 \\
        MAGE & ViT-L & 9.10 & - \\
        MAGE（弱增强） & ViT-L & 7.04 & - \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/屏幕截图 2025-12-25 101208.png}
    \caption*{论文3 figure 2}
\end{figure}
 
•	关键结论：MAGE 生成质量远超 MAE（细节丰富 vs 模糊），ViT-L 模型 FID 接近有类别生成最优水平（~6 FID）；不同随机种子可生成多样化结果。

3. 表征学习性能


\begin{table}[htbp]
    \centering
    \caption*{论文 3 表 2 线性探测准确率对比表} 
    \begin{tabular}{lllr} 
        \toprule
        方法 & 模型 & 参数量 & 准确率 \\
        \midrule
        MAE & ViT-L & 304M & 75.8\% \\
        MoCo v3 & ViT-L & 304M & 77.6\% \\
        MAGE & ViT-L & 24M+304M & 78.9\% \\
        MAGE-C & ViT-L & 24M+304M & 80.9\% \\
        \bottomrule
    \end{tabular}
\end{table}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/屏幕截图 2025-12-25 104045.png}
    \caption*{论文 3 figure 5迁移学习性能对比图}
\end{figure}
•	关键结论：MAGE-C ViT-L 线性探测准确率达 80.9\%，超越所有 MIM 方法与对比学习基线；迁移学习在 8 个下游数据集（如 Birds、CIFAR100）中，6 个优于 MAE 与 SimCLR，对领域偏移鲁棒。

4. 其他任务表现

•	少样本学习：MAGE-C ViT-L 在每类 5-25 张图像设置下，准确率达71.9\%-74.2\%，超越多裁剪增强的 MSN。

•	有类别条件生成：ViT-B 模型 FID=6.93，与 MaskGIT（6.18）性能相当，仅训练解码器无需微调编码器。

•	图像编辑：支持修复、扩绘、裁剪恢复等任务，重建结果细节丰富。








\subsection{自选论文题目}
\subsubsection{论文动机}

1.解决深层网络的梯度消失与特征传播不畅问题：随着 CNN 层数增加，输入信息和梯度在多层传递中易 “消失”，导致网络难以训练。尽管 ResNet、Highway Networks 等通过跳接缓解了这一问题，但特征通过求和组合仍可能阻碍信息流，DenseNet 通过 “每层与所有前序层直接连接 + 特征拼接” 的模式，让梯度和特征直接传递，彻底强化信息流动。

2.减少网络参数冗余，提升参数效率：传统 CNN 及 ResNet 中，每层需学习冗余特征图以保留前序信息，导致模型庞大。DenseNet 采用 “狭窄层 + 特征复用” 设计，每层仅新增少量特征图（由增长率 k 控制），所有前序特征图可直接被后序层调用，无需重复学习，大幅减少参数数量。

3.强化特征复用与隐式深度监督：现有网络未充分利用不同层的特征互补性，而 DenseNet 的密集连接让各层能直接访问所有前序特征，促进特征多样化复用；同时，损失函数的梯度可直接传递至所有层，形成隐式深度监督，提升特征判别力，减少过拟合。


\subsubsection{实验结果}

作者在 CIFAR-10、CIFAR-100、SVHN 和 ImageNet 四个基准数据集上验证了 DenseNet 的有效性。

\textbf{CIFAR \& SVHN：}
如表1所示，DenseNet 在参数量大幅减少的情况下取得了超越当时最优方法的性能。例如，DenseNet-BC ($L=190$, $k=40$) 在 CIFAR-10+ 上错误率仅为 3.46\%，显著优于同等深度的宽残差网络。在无数据增强的设置下，DenseNet 表现出更强的抗过拟合能力，错误率相对降低约 30\%。

\begin{table}[h]
  \centering
  \begin{tabular}{@{}lcccc@{}}
    \hline \hline
    方法 & 参数量 & C10+ & C100+ & SVHN \\
    \hline
    Wide ResNet (28层) & 36.5M & 4.17 & 20.50 & - \\
    ResNet-1001 (预激活) & 10.2M & 4.62 & 22.71 & - \\
    DenseNet-BC ($L=190$, $k=40$) & \textbf{25.6M} & \textbf{3.46} & \textbf{17.18} & - \\
    DenseNet ($L=100$, $k=24$) & 27.2M & 3.74 & 19.25 & \textbf{1.59} \\
    \hline \hline
  \end{tabular}
  \caption{DenseNet 在 CIFAR 和 SVHN 数据集上的错误率（\%）对比}
  \label{tab:densenet_results}
\end{table}

\textbf{ImageNet：}
在 ImageNet 上，DenseNet-BC 在参数量和计算量（FLOPs）显著少于 ResNet 的情况下取得了相当甚至更优的性能（图2）。例如，DenseNet-201（约20M参数）与 ResNet-101（约45M参数）错误率相当，表明 DenseNet 具有更高的参数效率。

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/densenet_imagenet.png}  
    \caption{\small DenseNet 与 ResNet 在 ImageNet 上的单裁剪 top-1 错误率对比（左：参数量，右：FLOPs）}
    \label{fig:densenet_vs_resnet}
\end{figure}

\textbf{特征重用分析：}
作者通过可视化各层权重发现，DenseNet 中的层确实广泛利用了之前层的特征（图5）。深层网络仍能直接使用浅层特征，体现了特征重用的有效性。过渡层输出的特征权重普遍较低，说明其存在冗余，这也解释了压缩机制的有效性。

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/feature_reuse_heatmap.png}  
    \caption{\small 训练后的 DenseNet 中各卷积层权重的平均绝对值热图，反映了层与层之间的特征依赖关系。}
    \label{fig:feature_reuse}
\end{figure}









\section{思考与理解：}

\subsection{联系与区别}

\subsubsection{核心联系}

\begin{itemize}
    \item 均聚焦计算机视觉领域的\textbf{特征学习优化}，目标是提升网络对图像语义、细节的捕捉能力，适配图像分类、检测、分割等下游任务。
    \item 均针对深层网络的关键痛点提出解决方案：要么解决梯度消失/特征传播不畅（ResNet、DenseNet），要么优化计算效率（Swin Transformer），要么打破任务割裂（MAGE）。
    \item 均基于“增强层间信息交互”的核心思路：通过特殊连接方式（残差、密集）、注意力机制或掩码策略，强化层间信息流动，提升模型表达能力。
    \item 均在主流基准数据集（ImageNet、CIFAR系列）上验证效果，且均成为后续视觉模型的重要基础架构或参考范式。
\end{itemize}

\subsubsection{核心区别}

\paragraph{1. 核心目标与定位对比}
\vspace{5pt} 
\begin{table}[htbp]
    \centering
    \caption*{核心目标与定位对比}
    
    \begin{tabularx}{\linewidth}{>{\RaggedRight}X >{\RaggedRight}X >{\RaggedRight}X}
        \toprule  
        论文 & 核心目标 & 核心定位 \\
        \midrule  
        ResNet & 解决深层网络“退化问题”，让超深网络可训练 & 通用CNN骨干网络，奠定深层网络训练基础 \\
        DenseNet & 强化特征复用，减少参数冗余，提升参数效率 & 紧凑型CNN骨干，适合小数据集或资源受限场景 \\
        Swin Transformer & 适配视觉任务的Transformer架构，降低计算复杂度 & 通用视觉Transformer骨干，兼顾分类与密集预测（检测/分割） \\
        MAGE & 打破生成建模与表征学习的割裂，实现双任务统一 & 多任务统一框架，同时支持图像生成与自监督表征学习 \\
        \bottomrule  
    \end{tabularx}
\end{table}

\paragraph{2. 核心创新与架构设计对比}
\vspace{5pt}
\begin{table}[htbp]
    \centering
    \caption*{核心创新与架构设计对比}

    \begin{tabularx}{\linewidth}{>{\RaggedRight}X >{\RaggedRight}X >{\RaggedRight}X >{\RaggedRight}X}
        \toprule
        论文 & 核心创新 & 架构类型 & 关键设计 \\
        \midrule
        ResNet & 残差连接（Residual Connection） & CNN & 恒等映射+残差学习，加法融合前序特征，梯度直接回传 \\
        DenseNet & 密集连接（Dense Connection） & CNN & 每层与所有前序层直接连接，特征拼接复用，引入“增长率k” \\
        Swin Transformer & 移位窗口自注意力（Shifted Window Attention） & Transformer & 局部窗口内计算注意力（线性复杂度），窗口移位实现跨窗口交互 \\
        MAGE & 可变掩码率+语义令牌化 & Transformer（编码器-解码器） & 掩码率0.5-1动态采样；输入/输出为VQGAN量化令牌；可选对比损失 \\
        \bottomrule
    \end{tabularx}
\end{table}

\paragraph{3. 适用场景与核心优势对比}
\vspace{5pt}
\begin{table}[htbp]
    \centering
    \caption*{适用场景与核心优势对比}
    \begin{tabularx}{\linewidth}{>{\RaggedRight}X >{\RaggedRight}X >{\RaggedRight}X >{\RaggedRight}X}
        \toprule
        论文 & 优势场景 & 核心优势 & 局限 \\
        \midrule
        ResNet & 各类视觉基础任务，需快速落地的场景 & 结构简单、训练稳定、泛化性强 & 特征复用效率有限，深层仍有冗余 \\
        DenseNet & 小数据集、资源受限场景，紧凑模型需求 & 参数效率高、特征复用充分、过拟合风险低 & 密集连接导致内存消耗较高，推理略慢 \\
        Swin Transformer & 高分辨率图像任务、密集预测（检测/分割） & 长距离依赖捕捉强，计算复杂度与图像尺寸线性相关 & 需更多数据训练，小数据集适配性一般 \\
        MAGE & 同时兼顾生成（合成/编辑）与表征学习（分类/迁移）的场景 & 单模型支持双任务，生成质量与表征精度均达SOTA & 依赖预训练VQGAN令牌器，架构相对复杂 \\
        \bottomrule
    \end{tabularx}
\end{table}

\paragraph{4. 任务覆盖范围对比}
\vspace{5pt}
\begin{table}[htbp]
    \centering
    \caption*{任务覆盖范围对比}
    \begin{tabularx}{\linewidth}{>{\RaggedRight}X >{\RaggedRight}X >{\RaggedRight}X}
        \toprule
        论文 & 主要支持任务 & 任务割裂情况 \\
        \midrule
        ResNet & 图像分类、检测、分割等判别式任务 & 仅支持表征学习，不涉及生成任务 \\
        DenseNet & 同ResNet，侧重判别式任务的紧凑模型需求 & 仅支持表征学习 \\
        Swin Transformer & 判别式任务（分类、检测、分割）+ 部分生成任务（需适配） & 以判别式任务为主，生成任务需额外适配 \\
        MAGE & 生成任务（无类别/有类别生成、图像编辑）+ 表征学习（线性探测、少样本/迁移学习） & 原生支持双任务统一，无需独立训练 \\
        \bottomrule
    \end{tabularx}
\end{table}

\subsubsection{关键演进逻辑}
从架构演进来看，四篇论文呈现“从CNN到Transformer、从单任务到多任务”的趋势：
\begin{enumerate}
    \item ResNet（2015）：解决CNN深层训练的“退化问题”，开启超深网络时代；
    \item DenseNet（2016）：在ResNet基础上强化特征复用，优化参数效率，提出紧凑架构设计；
    \item Swin Transformer（2021）：将Transformer适配视觉任务，解决全局注意力的高复杂度问题，适配密集预测；
    \item MAGE（2023）：打破生成与表征学习的任务割裂，实现单模型双任务SOTA，探索多任务统一范式。
\end{enumerate}




\subsection{尚未解决的问题}




\subsection{未来研究趋势}

通过对ResNet、DenseNet、Swin Transformer和MAGE四篇代表性论文的分析，我们可以看到图像分类领域正处在快速发展期，研究的焦点已从单纯的提升模型精度，转向更广泛的议题。综合来看，未来的研究趋势将主要集中在以下几个方面。

\subsubsection*{1. 模型架构的融合与演进}
早期的成功模型，如ResNet通过引入“跳跃连接”有效解决了深度网络的训练难题。随后的DenseNet更进一步，通过将每一层与之前所有层相连，最大化地实现了特征重用。近年来，Swin Transformer等视觉Transformer模型，通过引入“滑动窗口”（shifted windows）等机制，在保持强大全局建模能力的同时，也拥有了类似CNN的层次化结构和计算效率。

一个明显的趋势是，未来的模型架构不会非此即彼，而是会走向融合。研究者们正在尝试将CNN擅长的局部特征提取、平移不变性（即图像中物体平移后仍能被识别）等优点，与Transformer模型强大的长距离依赖建模能力结合起来，创造出性能更强、效率更高的混合模型。

\subsubsection*{2. 学习方式的变革：迈向更通用的视觉智能}
传统的深度学习严重依赖大量人工标注的数据，这成本高昂且难以扩展到所有领域。未来的一个重要方向是让模型能够从海量未标注的图片中自行学习。Swin Transformer的成功就部分得益于在大规模无标签数据上的预训练。而MAGE这篇论文则展示了一个更激动人心的思路：它使用同一个模型框架，既能完成“掩码图像重建”（像拼图一样补全被遮挡的图片部分）这样的无监督任务，也能直接用于图像分类和生成。这种“一模型多用”的设计，让模型学习到的知识更加通用和健壮。

这种学习范式的转变，其最终目标是发展出“视觉基础模型”。这样的模型在经过海量数据预训练后，能够仅凭少量示例（小样本学习）甚至无需示例（零样本学习），就能快速适应到新的、未见过的视觉任务中。

\subsubsection*{3. 面向真实世界的挑战：让模型更实用、更可靠}
在实验室的标准数据集（如ImageNet）上取得高分只是第一步。要让AI真正落地，未来的研究必须直面真实世界的复杂性：

\begin{itemize}
    \item \textbf{识别更精细的类别：} 未来的分类不仅要区分“猫”和“狗”，还要能区分不同品种的猫（细粒度识别）。同时，现实世界的数据往往“长尾分布”，即少数类别（如“猫”“狗”）样本极多，而大多数类别（如“雪豹”）样本很少，如何让模型在这些“尾巴”类别上也表现良好，是一个关键挑战。
    \item \textbf{知道自己“不知道”：} 一个可靠的系统必须能识别出那些它从未学过、或与所学知识差异巨大的输入（分布外检测）。例如，一个用于识别动物的模型，当输入一张汽车图片时，它应该给出“不确定”或“未知”的答案，而不是强行归类。
    \item \textbf{让决策过程更透明：} 在医疗、自动驾驶等关键领域，我们不仅需要模型给出答案，还需要知道它“为什么”给出这个答案。因此，提升模型的可解释性，让它的决策依据对人类而言是清晰可理解的，将是未来不可或缺的一环。
\end{itemize}

\subsubsection*{4. 追求高效：让强大的模型触手可及}
随着模型越来越庞大（例如拥有数十亿参数），对计算资源和能耗的需求也急剧增加。为了让先进的AI技术能在手机、摄像头等资源有限的边缘设备上运行，模型的“瘦身”和加速是永恒的主题：

\begin{itemize}
    \item \textbf{设计更高效的架构：} 这包括设计参数量更少、计算速度更快的网络层和模块。
    \item \textbf{算法与硬件的协同设计：} 为特定的高效算法（如脉冲神经网络，一种模拟生物神经元工作方式、能效极高的模型）设计专用芯片，可以成百上千倍地降低功耗，这对于物联网和移动设备至关重要。
    \item \textbf{优化训练过程：} 训练大模型需要成千上万个处理器协作。如何设计高效的通信和任务调度系统，确保这些处理器能全力工作而不互相等待，是支撑未来AI发展的底层技术关键。
\end{itemize}

总而言之，图像分类的未来，正从一个相对独立的“分类”任务，演进为一个探索如何构建通用、高效、可靠且易于理解的视觉智能系统的宏大课题。这不仅需要算法创新，也需要在数据利用、硬件适配乃至安全性、伦理性等更广阔的维度上进行深入思考。



{\small
\bibliography{longstrings,Cmm}
\bibliographystyle{unsrt}
}

\end{document}
