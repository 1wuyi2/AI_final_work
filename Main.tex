% https://www.overleaf.com/read/jydxqkkkskzp
% https://github.com/MCG-NKU/NSFC-LaTex
% by Ming-Ming Cheng https://mmcheng.net

%\documentclass[12pt]{article}
\documentclass[12pt,AutoFakeBold]{article}
\usepackage[UTF8]{ctex}
\usepackage{nsfc}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{caption} 

\newcommand{\cmm}[1]{\textcolor[rgb]{0,0.6,0}{CMM: #1}}
\newcommand{\todo}[1]{{\textcolor{red}{\bf [#1]}}}
\newcommand{\myPara}[1]{\paragraph{#1：}}

% yaxing: begin
\usepackage[resetlabels]{multibib}
\newcites{sec}{Reference}
\usepackage{color}

\newcommand{\upcite}[1]{{\textsuperscript{\cite{#1}}}}
\newcommand{\sprod}[2]{\left\langle #1,#2 \right\rangle}
\newcommand{\minisection}[1]{\vspace{0.04in}\noindent {\bf #1}\ \ }


%  \iffalse
\newcommand{\revision}[1]{{\color{blue} #1}}
\graphicspath{{figures/}}


\begin{document}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\linewidth]{figures/logo.jpeg}
\end{figure}

\vspace{1cm}
\hrulefill
    \begin{center}
        \Huge\textbf{《人工智能导论》探究报告\\ 图像超分\textcolor{red}{为例} }
     
    \end{center}   

\hrulefill
\vspace{1cm}

\begin{table}[h]
    \centering
    \large
    \begin{tabular}{ll}
    \textbf{姓名：谭锦睿   \quad  学号:2412771} \quad 论文1  \\
    \textbf{姓名：高一飞    \quad  学号:0000000} \quad 论文2 \\
    \textbf{姓名：王毅豪    \quad  学号:2412509} \quad 论文3 \\
    \end{tabular}
\end{table}

\newpage

%%%%%%%%% TITLE

\title{\kaishu 作业正文}

\maketitle




\section{问题描述（\revision{文字描述与形式化定义，组员共同完成}）}


\section{核心内容（\revision{三篇论文三位组员分别负责，拓展阅读论文组员共同选题和完成，描述论文动机，方法，实验结果等关键内容}）：}

\subsection{规定论文题目1}
\subsubsection{论文动机}

如何使用插入图像：


\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/teaser.jpg}  
    %\vspace{-6mm}
    \caption{\small 示意图}
    \label{fig:framework}
    %\vspace{-15pt}
\end{figure*}



\subsubsection{方法}

如何使用公式：

\vspace{-3pt}
\begin{equation}
 \mathcal{L}_{kdl}^{e}= \sum _{l} \gamma_{l} \left \| D_{T}(\mathbf{x_{t}^{1}})_{l} -  E_{S}(\mathbf{x_{t}^{1}} )_{l}  \right \|_{1}
\label{eq:kdl_e}
\end{equation} 

\subsubsection{实验结果}

如何插入表格：

\begin{table}[h]
  \centering
  \begin{tabular}{@{}lc@{}}
    \hline    \hline

    Method & Frobnability \\
    \hline
    Theirs & Frumpy \\
        \hline

    Yours & Frobbly \\
        \hline

    Ours & Makes one's heart Frob\\
    \hline    \hline

  \end{tabular}
  \caption{Results.   Ours is better.}
  \label{tab:example}
\end{table}
\subsection{规定论文题目2}

\subsection{MAGE: MAsked Generative Encoder to UnifyRepresentation Learning and Image Synthesi}
\subsubsection{论文动机}
1.	任务割裂问题：生成建模与表征学习是计算机视觉核心任务，但长期独立训练，既浪费互补潜力，又增加训练维护成本。

2.	结构差异挑战：生成是 “低维输入→高维输出”，表征学习是 “高维图像→低维嵌入”，计算机视觉领域缺乏类似 NLP 中 BERT 的统一框架。

3.	现有 MIM 局限：基于掩码图像建模（MIM）的表征方法采用像素级重建损失，导致图像重建模糊，无法兼顾生成质量。
\subsubsection{方法}
1. 核心创新点

•	可变掩码率策略：掩码率从 0.5-1 的截断高斯分布采样，高掩码率（接近 1）适配生成任务（100 \% 掩码图像重建），低掩码率适配表征学习（0\% 掩码图像编码），共享架构与训练方案。

•	语义令牌化：用预训练 VQGAN 将图像量化为 16×16 语义令牌，输入与重建目标均为令牌（非像素），解决重建模糊问题。

2. 模型架构与训练
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/屏幕截图 2025-12-25 101129.png}
    \caption{论文 3 figure 3 }
\end{figure}
 
•	架构：ViT 编码器 - 解码器，输入令牌经掩码与丢弃后，拼接 “伪类别令牌”，编码器输出用图像专属类别令牌填充，解码器重建原始令牌。

•	损失函数：核心为重建交叉熵损失（仅优化被掩码令牌），可选添加对比损失（MAGE-C），无需额外数据增强。

•	生成推理：20 步迭代解码，按余弦调度替换低置信度掩码令牌。

3. 预训练设置

•	输入分辨率 256×256，令牌序列长度 256，预训练 ViT-B/ViT-L 模型。

•	优化器 AdamW，训练 1600 个 epoch，余弦学习率调度 + 80 个 epoch 预热。

\subsubsection{实验结果}

1. 核心性能总览

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/屏幕截图 2025-12-25 101107.png}
    \caption*{论文 3 figure 1}
\end{figure}

•	论文图 1 直观展示 MAGE 在双任务的 SOTA 地位：ViT-L 模型无类别生成 FID=9.10，线性探测准确率 = 78.9\%，大幅超越 MaskGIT、MAE 等基线。

2. 无类别条件图像生成

\begin{table}[htbp]
    \centering
    \caption*{论文3 表 1 无类别条件图像生成性能对比}
    \begin{tabular}{llrr}
        \toprule
        方法 & 模型 & FID & IS \\
        \midrule
        MaskGIT（此前 SOTA） & BERT & 20.68 & 42.08 \\
        MAGE & ViT-B & 11.11 & 81.17 \\
        MAGE & ViT-L & 9.10 & - \\
        MAGE（弱增强） & ViT-L & 7.04 & - \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/屏幕截图 2025-12-25 101208.png}
    \caption*{论文3 figure 2}
\end{figure}
 
•	关键结论：MAGE 生成质量远超 MAE（细节丰富 vs 模糊），ViT-L 模型 FID 接近有类别生成最优水平（~6 FID）；不同随机种子可生成多样化结果。

3. 表征学习性能


\begin{table}[htbp]
    \centering
    \caption*{论文 3 表 2 线性探测准确率对比表} 
    \begin{tabular}{lllr} 
        \toprule
        方法 & 模型 & 参数量 & 准确率 \\
        \midrule
        MAE & ViT-L & 304M & 75.8\% \\
        MoCo v3 & ViT-L & 304M & 77.6\% \\
        MAGE & ViT-L & 24M+304M & 78.9\% \\
        MAGE-C & ViT-L & 24M+304M & 80.9\% \\
        \bottomrule
    \end{tabular}
\end{table}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/屏幕截图 2025-12-25 104045.png}
    \caption*{论文 3 figure 5迁移学习性能对比图}
\end{figure}
•	关键结论：MAGE-C ViT-L 线性探测准确率达 80.9\%，超越所有 MIM 方法与对比学习基线；迁移学习在 8 个下游数据集（如 Birds、CIFAR100）中，6 个优于 MAE 与 SimCLR，对领域偏移鲁棒。

4. 其他任务表现

•	少样本学习：MAGE-C ViT-L 在每类 5-25 张图像设置下，准确率达71.9\%-74.2\%，超越多裁剪增强的 MSN。

•	有类别条件生成：ViT-B 模型 FID=6.93，与 MaskGIT（6.18）性能相当，仅训练解码器无需微调编码器。

•	图像编辑：支持修复、扩绘、裁剪恢复等任务，重建结果细节丰富。

\subsection{自选论文题目 （\revision{组员共同完成}）}
\subsubsection{论文动机}
1.	解决深层网络的梯度消失与特征传播不畅问题：随着 CNN 层数增加，输入信息和梯度在多层传递中易 “消失”，导致网络难以训练。尽管 ResNet、Highway Networks 等通过跳接缓解了这一问题，但特征通过求和组合仍可能阻碍信息流，DenseNet 通过 “每层与所有前序层直接连接 + 特征拼接” 的模式，让梯度和特征直接传递，彻底强化信息流动。

2.	减少网络参数冗余，提升参数效率：传统 CNN 及 ResNet 中，每层需学习冗余特征图以保留前序信息，导致模型庞大。DenseNet 采用 “狭窄层 + 特征复用” 设计，每层仅新增少量特征图（由增长率 k 控制），所有前序特征图可直接被后序层调用，无需重复学习，大幅减少参数数量。

3.	强化特征复用与隐式深度监督：现有网络未充分利用不同层的特征互补性，而 DenseNet 的密集连接让各层能直接访问所有前序特征，促进特征多样化复用；同时，损失函数的梯度可直接传递至所有层，形成隐式深度监督，提升特征判别力，减少过拟合。



\section{思考与理解（\revision{联系与区别、尚未解决的问题、未来研究趋势等，组员共同完成}）：}

\subsection{联系与区别}

\subsubsection{核心联系}

\begin{itemize}
    \item 均聚焦计算机视觉领域的\textbf{特征学习优化}，目标是提升网络对图像语义、细节的捕捉能力，适配图像分类、检测、分割等下游任务。
    \item 均针对深层网络的关键痛点提出解决方案：要么解决梯度消失/特征传播不畅（ResNet、DenseNet），要么优化计算效率（Swin Transformer），要么打破任务割裂（MAGE）。
    \item 均基于“增强层间信息交互”的核心思路：通过特殊连接方式（残差、密集）、注意力机制或掩码策略，强化层间信息流动，提升模型表达能力。
    \item 均在主流基准数据集（ImageNet、CIFAR系列）上验证效果，且均成为后续视觉模型的重要基础架构或参考范式。
\end{itemize}

\subsubsection{核心区别}

\paragraph{1. 核心目标与定位对比}
\begin{table}[htbp]
    \centering
    \caption*{核心目标与定位对比}
    \begin{tabular}{p{0.15\textwidth} p{0.35\textwidth} p{0.4\textwidth}}
        \toprule
        论文 & 核心目标 & 核心定位 \\
        \midrule
        ResNet & 解决深层网络“退化问题”，让超深网络可训练 & 通用CNN骨干网络，奠定深层网络训练基础 \\
        DenseNet & 强化特征复用，减少参数冗余，提升参数效率 & 紧凑型CNN骨干，适合小数据集或资源受限场景 \\
        Swin Transformer & 适配视觉任务的Transformer架构，降低计算复杂度 & 通用视觉Transformer骨干，兼顾分类与密集预测（检测/分割） \\
        MAGE & 打破生成建模与表征学习的割裂，实现双任务统一 & 多任务统一框架，同时支持图像生成与自监督表征学习 \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{2. 核心创新与架构设计对比}
\begin{table}[htbp]
    \centering
    \caption*{核心创新与架构设计对比}
    \begin{tabular}{p{0.15\textwidth} p{0.25\textwidth} p{0.2\textwidth} p{0.3\textwidth}}
        \toprule
        论文 & 核心创新 & 架构类型 & 关键设计 \\
        \midrule
        ResNet & 残差连接（Residual Connection） & CNN & 恒等映射+残差学习，加法融合前序特征，梯度直接回传 \\
        DenseNet & 密集连接（Dense Connection） & CNN & 每层与所有前序层直接连接，特征拼接复用，引入“增长率k” \\
        Swin Transformer & 移位窗口自注意力（Shifted Window Attention） & Transformer & 局部窗口内计算注意力（线性复杂度），窗口移位实现跨窗口交互 \\
        MAGE & 可变掩码率+语义令牌化 & Transformer（编码器-解码器） & 掩码率0.5-1动态采样；输入/输出为VQGAN量化令牌；可选对比损失 \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{3. 适用场景与核心优势对比}
\begin{table}[htbp]
    \centering
    \caption*{适用场景与核心优势对比}
    \begin{tabular}{p{0.15\textwidth} p{0.25\textwidth} p{0.25\textwidth} p{0.25\textwidth}}
        \toprule
        论文 & 优势场景 & 核心优势 & 局限 \\
        \midrule
        ResNet & 各类视觉基础任务，需快速落地的场景 & 结构简单、训练稳定、泛化性强 & 特征复用效率有限，深层仍有冗余 \\
        DenseNet & 小数据集、资源受限场景，紧凑模型需求 & 参数效率高、特征复用充分、过拟合风险低 & 密集连接导致内存消耗较高，推理略慢 \\
        Swin Transformer & 高分辨率图像任务、密集预测（检测/分割） & 长距离依赖捕捉强，计算复杂度与图像尺寸线性相关 & 需更多数据训练，小数据集适配性一般 \\
        MAGE & 同时兼顾生成（合成/编辑）与表征学习（分类/迁移）的场景 & 单模型支持双任务，生成质量与表征精度均达SOTA & 依赖预训练VQGAN令牌器，架构相对复杂 \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{4. 任务覆盖范围对比}
\begin{table}[htbp]
    \centering
    \caption*{任务覆盖范围对比}
    \begin{tabular}{p{0.15\textwidth} p{0.4\textwidth} p{0.35\textwidth}}
        \toprule
        论文 & 主要支持任务 & 任务割裂情况 \\
        \midrule
        ResNet & 图像分类、检测、分割等判别式任务 & 仅支持表征学习，不涉及生成任务 \\
        DenseNet & 同ResNet，侧重判别式任务的紧凑模型需求 & 仅支持表征学习 \\
        Swin Transformer & 判别式任务（分类、检测、分割）+ 部分生成任务（需适配） & 以判别式任务为主，生成任务需额外适配 \\
        MAGE & 生成任务（无类别/有类别生成、图像编辑）+ 表征学习（线性探测、少样本/迁移学习） & 原生支持双任务统一，无需独立训练 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{关键演进逻辑}
从架构演进来看，四篇论文呈现“从CNN到Transformer、从单任务到多任务”的趋势：
\begin{enumerate}
    \item ResNet（2015）：解决CNN深层训练的“退化问题”，开启超深网络时代；
    \item DenseNet（2016）：在ResNet基础上强化特征复用，优化参数效率，提出紧凑架构设计；
    \item Swin Transformer（2021）：将Transformer适配视觉任务，解决全局注意力的高复杂度问题，适配密集预测；
    \item MAGE（2023）：打破生成与表征学习的任务割裂，实现单模型双任务SOTA，探索多任务统一范式。
\end{enumerate}

\subsection{尚未解决的问题}


\subsection{未来研究趋势}

如何添加参考文献：

图像到图像转换基本思想是将给定源域图像映射到目标域，在此过程中输出图像具有两个特点：(1)保持输入图像结构、姿势信息；(2)获取目标域图像风格、属性、外表特征。图像到图像转换最早追溯到图像类比\upcite{hertzmann2001image}开始， Hertzmann等人\upcite{efros1999texture}提出单个输入-输出训练图像对的非参数纹理模型。 最近的方法\upcite{long2015fully}使用输入输出示例的数据集来学习使用卷积神经网络(CNN)的参数翻译函数。针对输出为RGB的图像到图像转换，近年来相关研究主要集中于基于生成对抗网络(GAN)的学习方法，其从数据要求角度通常分为二类：监督方式图像到图像转换和非监督方式图像到图像转换。监督方式图像到图像转换中，Phillip等人\upcite{pix2pix2017}基于U-Net构建一种包含编码器-解码器的生成器架构：Pix2pix，中科院团队\upcite{song2018geometry}在图像转换中引入关键点信息



{\small
\bibliography{longstrings,Cmm}
\bibliographystyle{unsrt}
}

\end{document}
